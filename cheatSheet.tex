\documentclass[a4paper]{article}

%% Language and font encodings
\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage[T1]{fontenc}

%% Sets page size and margins
\usepackage[a4paper,top=2cm,bottom=2cm,left=1cm,right=1cm,marginparwidth=1.75cm]{geometry}

%% Useful packages
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}

\begin{document}
    \begin{itemize}
        \item Linear/undetermined systems
        \begin{itemize}
            \item Matrix multiplication is \(\rightarrow \downarrow\).
            \item \textbf{Row echelon form} is the result of Gaussian elimination.
            \item \textbf{Pivots} are first nonzero value in a row.
            \item \textbf{Free variables} are those columns without pivots.
            \item \textbf{Dependent variables} are columns with pivots.
            \item \textbf{Rank} of a matrix is the number of pivots.
            \item \(Ax=b\) for multiple solutions is the \textbf{general (total) solution}: \(A(x_s+\beta x_n)=b\)
            
            where \(x_s\) is s.t. \(Ax_s=b\) and \(x_n\) s.t. \(Ax_n=0\)
            \item \textbf{Specific (particular) solutions} are solutions \(x_s\), and \textbf{vectors in the null space} are solutions \(x_n\).
            
            To find specific solutions, set free variables equal to 0 and solve for \(Ax_s=b\).
            \item Vectors form a basis for a \textbf{column space} of \(A\) if they're columns in \(A\) and linearly independent. \(\mathcal{C}(A)=\text{Span}(a_0\text{, ... , }a_n-1)\). So just the columns in the original matrix in which the pivots appear. Note: Also valid are columns from row echelon form, as they have the same span.
            
            Dimension is the number of dependent variables
            \item Vectors in a \textbf{row space} are column vectors. The rows in which the pivots appear (both in row echelon and initial, though we usually use row echelon) transversed.
            \item To find vectors in the \textbf{null space} set the first free variable to 1 and the second to 0 for the first vector and solve for \(Ax_n=0\), then flip for the second.
            
            Null space dimension is the number of columns - the number of pivots.
            \item \textbf{General solution} is \(x_s +\beta _kx_k\) all \(k\) vectors in the null space.
        \end{itemize}
        
        \item QR factorization:
            \begin{itemize}
                \item \textbf{Normal equation}: \(A^TA\hat{x}=A^Tb\) or \((A^TA)^{-1}A^Tb=\hat{x}\)
                
                Plug in \(A\) and \(b\) to find \(\hat{x}\) = best approximate solution (\textbf{linear least-squares solution}) 
                
                \item \textbf{Compute projection} of \( b \) onto \(A\), call it \(\hat{b}\): \(A(A^TA)^{−1}A^Tb=\hat{b}\).
                
                Note that this is just the \(\hat{x}\) from the normal equation multiplied by \(A\), so \(\hat{b}=A\hat{x}\).
                
                \item Orthonormal vectors: $$q_k=\frac{a^\bot _k}{\rho _{k,k}}=\frac{a^{\bot }_k}{\parallel a^{\bot }_k \parallel _2}$$ where
                    $$a^\bot _k = a_k − \rho _{0,k}q_0− \dots −\rho _{k−1,k}q_{k−1}$$
                    $$a^\bot _k=a_k−q^T_0a_kq_0−\dots −q^T_{k−1}a_kq_{k−1}$$
                    
                \item \(A=QR\) where \(Q=(q_0 | \dots | q_{n-1})\)
                
                    and \(R=
                        \begin{pmatrix}
                        \parallel a_0 \parallel _2 & q_0^Ta_1 & \dots & q_0^Ta_{n-1} \\
                        & \parallel a_1^{\bot } \parallel _2 & \ddots & \vdots \\
                        & & \ddots & q_{n-2}^Ta_{n-1} \\
                        0 & & & \parallel a^{\bot }_{n-1} \parallel _2
                        \end{pmatrix}
                    \). Basically any \(\rho _{k,k}=\parallel a_k^{\bot } \parallel _2 \),
                    \(\rho _{(i<k),k}=q_i^Ta_k\), and \(\rho _{(i>k),k}=0\).
                \item \(Q^TQ=I\)
            \end{itemize}
        \item Space spanned by vectors: \(A=\{ a_0|\text{...}|a_{n−1}\} \) where \(A\) is the space and \(a_k\) is a vector.
        \item \textbf{Eigenvalues} are scalars \(\lambda \). \(\lambda\) is an eigenvalue of \(A \iff Ax = \lambda x\) for some non-zero vector \(x\). So:
        
            For 2x2 \(M =
                \begin{pmatrix}
                    a-\lambda & b \\
                    c & d-\lambda 
                \end{pmatrix}\), and for 3x3 \( M =
                \begin{pmatrix}
                    a-\lambda & b & c \\
                    d & e-\lambda & f \\
                    g & h & i-\lambda 
                \end{pmatrix} \),
                
            and \( \text{det}(A-\lambda I)=0\).
        \item A vector \(x\) is an \textbf{eigenvector} of \(A\) if \(Ax=\lambda x\) where \(\lambda \) is some scalar. You get them by doing \( (A-\lambda I)x=0\) and solving for \(x\).
        \pagebreak
        \item Determinants:
            \begin{itemize}
                \item For 2x2 matrices 
                    \(M =
                    \begin{pmatrix}
                        a & b \\
                        c & d
                    \end{pmatrix}\),
                    \(\text{det}(M)=ad-bc \)
                \item For 3x3 matrices 
                    \(M =
                    \begin{pmatrix}
                        a & b & c \\
                        d & e & f \\
                        g & h & i
                    \end{pmatrix}\),
                    \(\text{det}(M)= a(ei-hf)-b(di-fg)+c(dh-eg) \), or
                    
                    \(\text{det}(M)= aei+bfg+cdh-(afh+bdi+ceg) \)
            \end{itemize}
        \item Inverses:
            \begin{itemize}
                \item For 2x2 matrices 
                    \(M^{-1} =
                    \begin{pmatrix}
                        a & b \\
                        c & d
                    \end{pmatrix}
                    ^{-1}=
                    \frac{1}{ad-bc}
                    \begin{pmatrix}
                        d & -b \\
                        -c & a
                    \end{pmatrix}
                    \)
                \item For 3x3 matrices and up don't try to be tricky. Use Gauss-Jordan.
            \end{itemize}
        \item \textbf{Diagonalization}: We have \(X^{-1}AX=D\) where:
        
        \[X= (x_0 | \text{...} | x_{n-1}) \text{ and } D =
        \begin{pmatrix}
            \lambda _0 & & 0 \\
            & \ddots & \\
            0 & & \lambda _{n-1}
        \end{pmatrix}
        \text{ where each } x_k \text{ and }\lambda _k \text{ are an eigenvector and its eigenvalue, respectively.}\]
        \item \textbf{LU Factorization}: \(U\) = result of Gaussian elimination, \(L\) is all the transformations you made combined. \(LU=A\)
        
        To do \(Ax=b\)...
        \[Ax=b\] \[LUx=b\] \[Ly=b \text{ and } Ux=y\]
        solve for \(y\) in the first equation, and then use \(y\) to solve for \(x\) in the second.
        \item Equivalent to "A is nonsingular"
            \begin{itemize}
                \item \(A\) is invertible.
                \item \(A^{-1}\) exists.
                \item \(AA^{-1}=A^{-1}A=I\).
                \item \(A\) represents a linear transformation that is a bijection.
                \item \(Ax=b\) has a unique solution for all \(b\in \mathbb{R} ^n\).
                \item \(Ax=0\) implies that x=0.
                \item \(Ax=e_j\) has a solution for all \(j\in \{0,\dots ,n-1\}\)
                \item The determinant of \(A\) is nonzero: \(det(A)\neq 0\).
                \item \(LU\) with partial pivoting does not break down.
                \item \(\mathcal{N}(A)={0}\).
                \item \(\mathcal{C}(A)=\mathbb{R}^n\).
                \item \(\mathcal{R}(A)=\mathbb{R}^n\).
                \item \(A\) has linearly independent columns.
                \item \(A\) has linearly independent rows.
            \end{itemize}
    \end{itemize}
\end{document}